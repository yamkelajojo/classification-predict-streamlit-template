{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46005e14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T09:24:53.643384Z",
     "start_time": "2021-06-11T09:24:53.622385Z"
    }
   },
   "source": [
    "# Regression Predict Student Solution\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "---\n",
    "### Honour Code\n",
    "\n",
    "I {**YOUR NAME, YOUR SURNAME**}, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "### Predict Overview: EA Twitter Sentiment classification.\n",
    "\n",
    "This process requires the user to input text (ideally a tweet relating to climate change), and will classify it according to whether or not they believe in climate change. Your company has been awarded the contract to:\n",
    "\n",
    "- 1. analyse the supplied data;\n",
    "- 2. identify potential errors in the data and clean the existing data set;\n",
    "- 3. determine if additional features can be added to enrich the data set;\n",
    "- 4. build a model that is capable of forecasting the three hourly demand shortfalls;\n",
    "- 5. evaluate the accuracy of the best machine learning model;\n",
    "- 6. determine what features were most important in the model’s prediction decision, and\n",
    "- 7. explain the inner working of the model to a non-technicsetal audience.\n",
    "\n",
    "Formally the problem statement\n",
    "\n",
    "This process requires the user to input text (ideally a tweet relating to climate change), and will classify it according to whether or not they believe in climate change.Below you will find information about the data source and a brief data description. You can have a look at word clouds and other general EDA on the EDA page, and make your predictions on the prediction page that you can navigate to in the sidebar.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05600c92",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Engineering</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997462e2",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to import, and briefly discuss, the libraries that will be used throughout your analysis and modelling. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c521ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import joblib,os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a6718",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to load the data from the `df_train` file into a DataFrame. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbb6c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:35.311495Z",
     "start_time": "2021-06-28T08:49:35.295494Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vectorizer\n",
    "#news_vectorizer = open(\"tfidfvect.pkl\",\"rb\")\n",
    "#tweet_cv = joblib.load(news_vectorizer) # loading your vectorizer from the pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c257fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(\"train.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81132ab3",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to perform an in-depth analysis of all the variables in the DataFrame. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e805134e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:52:37.824204Z",
     "start_time": "2021-06-28T08:52:37.811206Z"
    }
   },
   "outputs": [],
   "source": [
    "raw.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58260f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d4b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea26df",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795d0609",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_labels = {\n",
    "    '-1': '-1:Non-believer',\n",
    "    '0': '0:Not interested',\n",
    "    '1': '1:Neutral',\n",
    "    '2': '2:Out of topic'\n",
    "}\n",
    "\n",
    "ax = raw['sentiment'].value_counts().plot(kind='bar')\n",
    "unique_sentiments = raw['sentiment'].unique()\n",
    "ax.set_xticklabels([sentiment_labels.get(str(sentiment), 'Unknown') for sentiment in unique_sentiments])\n",
    "\n",
    "for i, v in enumerate(raw['sentiment'].value_counts()):\n",
    "    label = sentiment_labels.get(str(i), 'Unknown')\n",
    "    \n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e19118",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_list = []  \n",
    "\n",
    "# Loop over every cell in the \"message\" column\n",
    "for message in raw[\"message\"]:\n",
    "    if message: \n",
    "        tags = message.split() \n",
    "        for tag in tags:\n",
    "            tag = \"#\" + tag.strip(\",\")  \n",
    "            tag = tag.lower()  \n",
    "            hashtag_list.append(tag) \n",
    "\n",
    "print(hashtag_list[:20])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85085aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "hashtag_counts = Counter(hashtag_list)\n",
    "\n",
    "print(\"Total unique hashtags:\", len(hashtag_counts))\n",
    "\n",
    "print(\"unique hashtags:\")\n",
    "for tag, count in hashtag_counts.most_common(7):\n",
    "    print(tag, \"-\", count)\n",
    "#bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673a869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_counts = Counter(hashtag_list)\n",
    "top_hashtags = hashtag_counts.most_common(7)\n",
    "hashtags, counts = zip(*top_hashtags)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(hashtags, counts, color='red')\n",
    "plt.xlabel('Hashtags')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Top 7 Unique Hashtags')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f41d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7d4da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw[\"message\"][90])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b409acee",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdec4051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_handels(post):\n",
    "    return re.sub('@[^\\s]+',' ',post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c575221",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['message']= raw['message'].apply(remove_handels)\n",
    "raw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b4ff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "#subs_url = r'url-web'\n",
    "raw['message'] = raw['message'].replace(to_replace = pattern_url,value = \" \", regex = True)\n",
    "print(raw[\"message\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deed95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtages(post):\n",
    "    return re.sub('#[^\\s]+',' ',post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d8bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['message']= raw['message'].apply(remove_hashtages)\n",
    "print(raw[\"message\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7247622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(post):\n",
    "    return ''.join([l for l in post if l not in string.punctuation])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c987931",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw[\"message\"] = raw[\"message\"].apply(remove_punctuation)\n",
    "print(raw[\"message\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd6ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa93ec6",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. Data Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Data engineering ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to: clean the dataset, and possibly create new features - as identified in the EDA phase. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dealing with imbalances\n",
    "# Percentage of non spam emails in the dataset \n",
    "#len(not_spam)/(len(df))iuo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa18672",
   "metadata": {},
   "source": [
    "## Removing noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c2f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenazing\n",
    "raw = raw.drop([\"tweetid\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23b13f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw2 =raw\n",
    "raw2['message'] = raw2['message'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec912ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d7290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steming\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "raw2['message'] = raw2['message'].apply(lambda x: [stemmer.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw2['message'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22651fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de8e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens):    \n",
    "    return [t for t in tokens if t not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb797ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw2['message'] = raw2['message'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c640fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw2['message'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def mbti_lemma(words, lemmatizer):\n",
    "    return [lemmatizer.lemmatize(word) for word in words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c7e079",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw2['message'] = raw2['message'].apply(mbti_lemma, args=(lemmatizer, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11359d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf2708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw2['message'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffea7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts = raw['sentiment'].value_counts()\n",
    "\n",
    "minority_class = sentiment_counts.idxmin()\n",
    "minority_count = sentiment_counts.loc[minority_class]\n",
    "downsampled_raw = pd.concat([raw[raw['sentiment'] == minority_class]] +\n",
    "                            [raw[raw['sentiment'] == sentiment].sample(minority_count, replace=False) \n",
    "                             for sentiment in sentiment_counts.index if sentiment != minority_class])\n",
    "ax = downsampled_raw['sentiment'].value_counts().plot(kind='bar')\n",
    "ax.set_xticklabels([sentiment_labels.get(str(sentiment), 'Unknown') for sentiment in sentiment_counts.index])\n",
    "ax.set_ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a572f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = raw2['sentiment']\n",
    "\n",
    "# features\n",
    "X = raw2['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30fe31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf67794",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2d523",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "## 5. Modelling\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Modelling ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to create one or more regression models that are able to accurately predict the thee hour load shortfall. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677fbefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b966cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conveting word into numbers.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "betterVect = CountVectorizer(stop_words='english', \n",
    "                             min_df=2, \n",
    "                             max_df=0.5,\n",
    "                             ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f4587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_fitted = betterVect.fit_transform(X_train)\n",
    "X_test_counts = betterVect.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36faa8db",
   "metadata": {},
   "source": [
    "# logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89366b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the logistic regression model.\n",
    "lr = LogisticRegression(max_iter= 10000000000)\n",
    "lr.fit(X_train_fitted.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cfaaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr.predict(X_test_counts)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b5a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f097d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intercept\n",
    "lr.intercept_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5cf512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coefficients\n",
    "#coeff_df = pd.DataFrame(lr.coef_.T,X.columns,columns=['Coefficient'])\n",
    "#coeff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b1578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assessing Model Performance using the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5574263",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134b2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "labels = ['0: not interested', '1: nutural', \"-1: non beliver\", \"2:out of topic\"]\n",
    "\n",
    "pd.DataFrame(data=confusion_matrix(y_test, predictions), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed411c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Report in sklearn\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(y_test, predictions, target_names=['0: not interested', '1: nutural', \"-1: non beliver\", \"2:out of topic\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc0fd2a",
   "metadata": {},
   "source": [
    "# Decision Tree Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c58df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standarise the data\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# standard_scaler = StandardScaler()\n",
    "# X_test_counts = standard_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d073e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c15d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b530251",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "## 6. Model Performance\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model performance ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to compare the relative performance of the various trained ML models on a holdout dataset and comment on what model is the best and why. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a69b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose best model and motivate why it is the best choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c9d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.to_pickle('train3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55640a54-ebbf-42a2-a8f2-37c4a73992a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Assuming 'lr' is your trained Logistic Regression model\n",
    "joblib.dump(lr, 'train3.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad0c0d",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 7. Model Explanations\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model explanation ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to discuss how the best performing model works in a simple way so that both technical and non-technical stakeholders can grasp the intuition behind the model's inner workings. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff741c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discuss chosen methods logic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
